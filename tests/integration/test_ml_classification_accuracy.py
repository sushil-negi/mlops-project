"""
Integration tests for ML classification accuracy with real trained model
"""

import json
import os
import pickle
import sys

import pytest
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.pipeline import Pipeline

# Add the healthcare-ai source directory to the path
sys.path.insert(
    0, os.path.join(os.path.dirname(__file__), "../../models/healthcare-ai/src")
)

from healthcare_trained_engine import HealthcareTrainedEngine


class TestMLClassificationAccuracy:
    """Test ML classification accuracy with various queries"""

    @pytest.fixture
    def engine(self, trained_model_path):
        """Create engine with trained model"""
        return HealthcareTrainedEngine(str(trained_model_path))

    @pytest.mark.parametrize(
        "query,expected_category",
        [
            # ADL Mobility tests
            ("I need help getting out of bed", "adl_mobility"),
            ("Balance exercises for elderly", "adl_mobility"),
            ("Wheelchair transfer techniques", "adl_mobility"),
            # Senior Medication tests
            ("Pill reminder systems for memory loss", "senior_medication"),
            ("Managing diabetes medications", "senior_medication"),
            ("Medication organizer recommendations", "senior_medication"),
            # Mental Health tests
            ("Feeling very anxious lately", "mental_health_anxiety"),
            ("Depressed about my health", "mental_health_depression"),
            # Caregiver tests
            ("Need respite care for weekend", "caregiver_respite"),
            ("Burned out from caregiving", "caregiver_burnout"),
            # Disability tests
            ("Need adaptive eating utensils", "disability_equipment"),
            ("Workplace accommodation rights", "disability_rights"),
            # Crisis tests
            ("Want to hurt myself", "crisis_mental_health"),
        ],
    )
    def test_classification_accuracy(self, engine, query, expected_category):
        """Test that queries are classified correctly"""
        response = engine.generate_response(query)

        # Debug output for failing tests
        if query in ["Pill reminder systems for memory loss", "Workplace accommodation rights"]:
            print(f"\nDEBUG - Query: {query}")
            print(f"DEBUG - Response method: {response['method']}")
            print(f"DEBUG - Response category: {response['category']}")
            print(f"DEBUG - Response confidence: {response['confidence']}")

        # Crisis detection overrides ML classification
        if "hurt myself" in query.lower() or "suicide" in query.lower():
            assert response["method"] == "crisis_detection"
            assert response["category"] == "crisis_mental_health"
        else:
            # For non-crisis, check if category matches or is contextually overridden
            if response["method"] == "contextual_analysis":
                # Contextual override is acceptable - these provide better responses
                assert response["confidence"] == 0.95
                assert response["category"] == "contextual_override"
            else:
                # ML classification should be reasonably accurate
                # Allow for some flexibility in classification  
                assert response["method"] == "ml_model"
                # Category should be related to the expected one
                assert expected_category in response["category"] or response[
                    "category"
                ] in ["contextual_override", expected_category]

    def test_confidence_scores(self, engine):
        """Test that confidence scores are reasonable"""
        test_queries = [
            "I need help with mobility",
            "Medication management for seniors",
            "Feeling anxious about health",
            "Need respite care services",
            "Adaptive equipment for daily living",
        ]

        for query in test_queries:
            response = engine.generate_response(query)

            # Confidence should be between 0 and 1
            assert 0.0 <= response["confidence"] <= 1.0

            # Contextual overrides should have high confidence
            if response["method"] == "contextual_analysis":
                assert response["confidence"] == 0.95

            # Crisis detection should have maximum confidence
            if response["method"] == "crisis_detection":
                assert response["confidence"] == 1.0

    def test_ambiguous_query_handling(self, engine):
        """Test handling of ambiguous queries"""
        ambiguous_queries = [
            "I need help",
            "Having problems",
            "Not feeling well",
            "Family issues",
        ]

        for query in ambiguous_queries:
            response = engine.generate_response(query)

            # Should still provide a response
            assert response["response"]
            assert response["category"]

            # Confidence might be lower for ambiguous queries
            if response["method"] == "ml_model":
                # ML model might have lower confidence
                assert response["confidence"] >= 0.0

    def test_category_distribution(self, engine):
        """Test that model can classify into all categories"""
        category_test_queries = {
            "adl_mobility": "Help with walking and balance",
            "adl_self_care": "Bathing and dressing assistance",
            "senior_medication": "Organizing my medications",
            "senior_social": "Feeling lonely as a senior",
            "mental_health_anxiety": "Anxious about future",
            "mental_health_depression": "Feeling sad and hopeless",
            "caregiver_respite": "Need break from caregiving",
            "caregiver_burnout": "Exhausted from caring for parent",
            "disability_equipment": "Wheelchair recommendations",
            "disability_rights": "ADA accommodation at work",
        }

        categories_found = set()

        for expected_cat, query in category_test_queries.items():
            response = engine.generate_response(query)
            if response["method"] == "ml_model":
                categories_found.add(response["category"])

        # Should classify into multiple different categories
        assert len(categories_found) >= 5


class TestResponseQualityMetrics:
    """Test response quality metrics"""

    @pytest.fixture
    def engine(self, trained_model_path):
        """Create engine with trained model"""
        return HealthcareTrainedEngine(str(trained_model_path))

    def test_response_length(self, engine):
        """Test that responses are appropriately detailed"""
        queries = [
            "How do I transfer from bed to wheelchair?",
            "What medication reminders work best?",
            "I'm feeling overwhelmed as a caregiver",
        ]

        for query in queries:
            response = engine.generate_response(query)

            # Responses should be substantial
            assert len(response["response"]) > 100

            # But not too long
            assert len(response["response"]) < 1000

    def test_response_actionability(self, engine):
        """Test that responses contain actionable advice"""
        response = engine.generate_response(
            "My elderly father has trouble getting out of bed"
        )

        response_text = response["response"]

        # Should contain numbered steps or bullet points
        assert any(marker in response_text for marker in ["1)", "2)", "•", "-"])

        # Should contain specific actions
        actionable_keywords = ["Install", "Use", "Place", "Consider", "Contact", "Ask"]
        assert any(keyword in response_text for keyword in actionable_keywords)

    def test_response_safety_warnings(self, engine):
        """Test that responses include appropriate warnings"""
        response = engine.generate_response("I need help with wheelchair transfers")

        # Should include safety warning
        assert "⚠️" in response["response"]

        # Should mention professional consultation
        assert any(
            term in response["response"].lower()
            for term in ["professional", "healthcare provider", "therapist", "consult"]
        )

    def test_response_specificity(self, engine):
        """Test that responses are specific to the query"""
        # Test eating equipment
        eating_response = engine.generate_response(
            "I need adaptive equipment for eating"
        )
        assert (
            "eating" in eating_response["response"].lower()
            or "utensils" in eating_response["response"].lower()
            or "weighted" in eating_response["response"].lower()
        )

        # Test exercise
        exercise_response = engine.generate_response("What exercises help seniors?")
        assert any(
            term in exercise_response["response"].lower()
            for term in ["exercise", "walking", "balance", "strength", "tai chi"]
        )

        # Test medication
        med_response = engine.generate_response("How to remember medications?")
        assert any(
            term in med_response["response"].lower()
            for term in ["medication", "pill", "reminder", "organizer"]
        )


class TestPerformanceMetrics:
    """Test performance metrics of the system"""

    @pytest.fixture
    def engine(self, trained_model_path):
        """Create engine with trained model"""
        return HealthcareTrainedEngine(str(trained_model_path))

    def test_response_generation_time(self, engine):
        """Test that responses are generated quickly"""
        queries = [
            "I need help with mobility",
            "Medication management tips",
            "Feeling anxious about health",
        ]

        for query in queries:
            response = engine.generate_response(query)

            # Response should be generated in less than 100ms
            assert response["generation_time"] < 0.1

            # Cached responses should be even faster
            cached_response = engine.generate_response(query)
            assert cached_response.get("cached") == True
            assert cached_response["generation_time"] < 0.01

    def test_cache_effectiveness(self, engine):
        """Test that caching improves performance"""
        query = "How to manage multiple medications?"

        # First call - not cached
        response1 = engine.generate_response(query)
        time1 = response1["generation_time"]

        # Second call - should be cached
        response2 = engine.generate_response(query)
        time2 = response2["generation_time"]

        assert response2.get("cached") == True
        assert time2 < time1  # Cached should be faster

        # Verify cache stats
        stats = engine.get_stats()
        assert stats["cache_size"] > 0


if __name__ == "__main__":
    pytest.main([__file__, "-v"])
