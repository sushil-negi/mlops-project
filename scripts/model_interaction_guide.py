#!/usr/bin/env python3
"""
Complete guide for interacting with the demo LLM model
Shows various ways to use the trained model
"""

import json
import time

import requests


class ModelInteractionGuide:
    """Guide for interacting with the deployed LLM model"""

    def __init__(self):
        self.model_url = "http://localhost:8080"
        self.model_info = None

    def check_model_status(self):
        """Check if the model is running and healthy"""

        print("ğŸ” Checking model status...")

        try:
            # Health check
            health = requests.get(f"{self.model_url}/health").json()
            print(f"âœ… Model Status: {health['status']}")
            print(f"ğŸ¤– Model Loaded: {health['model_loaded']}")
            print(f"ğŸ“ Version: {health['version']}")

            # Model info
            info = requests.get(f"{self.model_url}/model/info").json()
            self.model_info = info
            print(f"ğŸ“Š Parameters: {info['parameters']:,}")
            print(f"ğŸ’» Device: {info['device']}")

            return True

        except requests.exceptions.ConnectionError:
            print("âŒ Model is not running. Start with: docker compose up -d")
            return False
        except Exception as e:
            print(f"âŒ Error: {e}")
            return False

    def basic_text_generation(self):
        """Demonstrate basic text generation"""

        print("\n" + "=" * 60)
        print("ğŸ’¬ 1. BASIC TEXT GENERATION")
        print("=" * 60)

        prompts = [
            "The future of artificial intelligence is",
            "Our company just launched",
            "Machine learning helps businesses",
            "Today I learned about",
            "The best way to implement AI is",
        ]

        for i, prompt in enumerate(prompts, 1):
            print(f"\nğŸ¯ Example {i}: '{prompt}'")

            response = requests.post(
                f"{self.model_url}/generate",
                json={"text": prompt, "max_length": 50, "temperature": 0.7},
            )

            if response.status_code == 200:
                result = response.json()
                print(f"ğŸ“ Output: {result['generated_text'][0]}")
                print(f"â±ï¸  Time: {result['generation_time']:.2f}s")
            else:
                print(f"âŒ Error: {response.text}")

    def parameter_exploration(self):
        """Show how different parameters affect generation"""

        print("\n" + "=" * 60)
        print("ğŸ›ï¸  2. PARAMETER EXPLORATION")
        print("=" * 60)

        base_prompt = "Exciting news about our AI product"

        # Temperature variations
        print("\nğŸŒ¡ï¸  Temperature Effects:")
        for temp in [0.3, 0.7, 1.0]:
            print(f"\nTemperature {temp} (lower = more focused):")

            response = requests.post(
                f"{self.model_url}/generate",
                json={"text": base_prompt, "max_length": 40, "temperature": temp},
            )

            if response.status_code == 200:
                result = response.json()
                print(f"ğŸ“ {result['generated_text'][0]}")

        # Length variations
        print("\nğŸ“ Length Effects:")
        for length in [20, 40, 80]:
            print(f"\nMax length {length}:")

            response = requests.post(
                f"{self.model_url}/generate",
                json={"text": base_prompt, "max_length": length, "temperature": 0.7},
            )

            if response.status_code == 200:
                result = response.json()
                print(f"ğŸ“ {result['generated_text'][0]}")

    def social_media_use_cases(self):
        """Demonstrate social media content generation"""

        print("\n" + "=" * 60)
        print("ğŸ“± 3. SOCIAL MEDIA CONTENT GENERATION")
        print("=" * 60)

        use_cases = [
            {
                "name": "Product Launch",
                "prompt": "Excited to announce our new",
                "params": {"max_length": 60, "temperature": 0.8},
            },
            {
                "name": "Tech Update",
                "prompt": "Breaking news in AI:",
                "params": {"max_length": 50, "temperature": 0.6},
            },
            {
                "name": "Company Update",
                "prompt": "Our team just achieved",
                "params": {"max_length": 55, "temperature": 0.7},
            },
            {
                "name": "Educational Content",
                "prompt": "Did you know that machine learning",
                "params": {"max_length": 70, "temperature": 0.5},
            },
        ]

        for case in use_cases:
            print(f"\nğŸ¯ {case['name']}:")
            print(f"   Prompt: '{case['prompt']}'")

            response = requests.post(
                f"{self.model_url}/generate",
                json={"text": case["prompt"], **case["params"]},
            )

            if response.status_code == 200:
                result = response.json()
                print(f"   ğŸ“ Generated: {result['generated_text'][0]}")
                print(f"   â±ï¸  Time: {result['generation_time']:.2f}s")

    def batch_generation(self):
        """Show how to generate multiple responses"""

        print("\n" + "=" * 60)
        print("ğŸ“¦ 4. BATCH GENERATION")
        print("=" * 60)

        prompts = [
            "Our AI startup is revolutionizing",
            "The future of work includes",
            "Technology trends for 2024:",
        ]

        print("ğŸ”„ Generating multiple responses...")

        results = []
        for i, prompt in enumerate(prompts, 1):
            print(f"\nğŸ“ Generating {i}/{len(prompts)}: '{prompt}'")

            response = requests.post(
                f"{self.model_url}/generate",
                json={"text": prompt, "max_length": 60, "temperature": 0.7},
            )

            if response.status_code == 200:
                result = response.json()
                results.append(
                    {
                        "prompt": prompt,
                        "output": result["generated_text"][0],
                        "time": result["generation_time"],
                    }
                )
                print(f"   âœ… Generated in {result['generation_time']:.2f}s")

        # Show summary
        print(f"\nğŸ“Š Batch Summary:")
        print(f"   Total responses: {len(results)}")
        print(f"   Average time: {sum(r['time'] for r in results) / len(results):.2f}s")

        return results

    def interactive_chat(self):
        """Demonstrate interactive conversation"""

        print("\n" + "=" * 60)
        print("ğŸ’¬ 5. INTERACTIVE CHAT MODE")
        print("=" * 60)

        print("ğŸ¤– Starting interactive session...")
        print("ğŸ’¡ Type 'quit' to exit")
        print("-" * 40)

        conversation_history = []

        while True:
            try:
                user_input = input("\nğŸ§‘ You: ")

                if user_input.lower() in ["quit", "exit", "q"]:
                    print("ğŸ‘‹ Goodbye!")
                    break

                if not user_input.strip():
                    continue

                # Generate response
                print("ğŸ¤– Thinking...")

                response = requests.post(
                    f"{self.model_url}/generate",
                    json={"text": user_input, "max_length": 60, "temperature": 0.7},
                )

                if response.status_code == 200:
                    result = response.json()
                    bot_response = result["generated_text"][0]

                    print(f"ğŸ¤– Bot: {bot_response}")
                    print(f"â±ï¸  ({result['generation_time']:.2f}s)")

                    conversation_history.append(
                        {
                            "user": user_input,
                            "bot": bot_response,
                            "timestamp": time.time(),
                        }
                    )
                else:
                    print(f"âŒ Error: {response.text}")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"âŒ Error: {e}")

        return conversation_history

    def curl_examples(self):
        """Show curl command examples"""

        print("\n" + "=" * 60)
        print("ğŸŒ 6. CURL COMMAND EXAMPLES")
        print("=" * 60)

        examples = [
            {
                "name": "Basic Generation",
                "command": """curl -X POST http://localhost:8080/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "text": "Our AI product helps",
    "max_length": 50,
    "temperature": 0.7
  }' """,
            },
            {"name": "Health Check", "command": "curl http://localhost:8080/health"},
            {"name": "Model Info", "command": "curl http://localhost:8080/model/info"},
            {
                "name": "High Temperature (Creative)",
                "command": """curl -X POST http://localhost:8080/generate \\
  -H "Content-Type: application/json" \\
  -d '{
    "text": "Innovation in tech",
    "max_length": 40,
    "temperature": 1.0
  }' """,
            },
        ]

        for example in examples:
            print(f"\nğŸ”§ {example['name']}:")
            print(f"```bash\n{example['command']}\n```")

    def python_sdk_example(self):
        """Show Python SDK usage"""

        print("\n" + "=" * 60)
        print("ğŸ 7. PYTHON SDK EXAMPLE")
        print("=" * 60)

        sdk_code = '''
import requests
import json

class DemoLLMClient:
    def __init__(self, base_url="http://localhost:8080"):
        self.base_url = base_url
    
    def generate(self, text, max_length=50, temperature=0.7):
        """Generate text completion"""
        response = requests.post(f"{self.base_url}/generate", json={
            "text": text,
            "max_length": max_length,
            "temperature": temperature
        })
        return response.json()
    
    def health_check(self):
        """Check model health"""
        return requests.get(f"{self.base_url}/health").json()
    
    def model_info(self):
        """Get model information"""
        return requests.get(f"{self.base_url}/model/info").json()

# Usage example
client = DemoLLMClient()

# Check if model is ready
health = client.health_check()
print(f"Model status: {health['status']}")

# Generate text
result = client.generate(
    text="The future of AI is",
    max_length=60,
    temperature=0.8
)
print(f"Generated: {result['generated_text'][0]}")
        '''

        print("ğŸ“ Save this as 'llm_client.py':")
        print("```python")
        print(sdk_code)
        print("```")

    def run_complete_demo(self):
        """Run the complete interaction demo"""

        print("ğŸš€ DEMO LLM MODEL INTERACTION GUIDE")
        print("=" * 60)

        if not self.check_model_status():
            return

        print("\nğŸ¯ This guide shows you how to:")
        print("1. Generate text with different parameters")
        print("2. Create social media content")
        print("3. Use the API programmatically")
        print("4. Build applications with the model")

        # Run demonstrations
        self.basic_text_generation()
        self.parameter_exploration()
        self.social_media_use_cases()

        # Show technical examples
        self.curl_examples()
        self.python_sdk_example()

        print("\n" + "=" * 60)
        print("âœ… INTERACTION GUIDE COMPLETE!")
        print("=" * 60)

        print("\nğŸ¯ Choose your interaction method:")
        print("1. Use curl commands for quick testing")
        print("2. Use Python requests for scripting")
        print("3. Build a web interface")
        print("4. Integrate into existing applications")

        print("\nğŸ’¡ Tips for better results:")
        print("- Use clear, specific prompts")
        print("- Adjust temperature: 0.3 (focused) to 1.0 (creative)")
        print("- Set appropriate max_length for your use case")
        print("- For production: add error handling and retries")


if __name__ == "__main__":
    guide = ModelInteractionGuide()

    import sys

    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        guide.run_complete_demo()
        guide.interactive_chat()
    else:
        guide.run_complete_demo()
